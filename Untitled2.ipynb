{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow import flags\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import util.text as tool\n",
    "import sys\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 docs / 1 save\n",
      "100000 docs / 99939 save\n",
      "사전단어수 : 49128\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "test_csv = '/home/beomgon2/medical_chart/movie/nsmc/test.csv'\n",
    "train_csv = '/home/beomgon2/medical_chart/movie/nsmc/train.csv'\n",
    "files = [train_csv, test_csv]\n",
    "\n",
    "# work_dir = '/home/beomgon2/medical_chart/movie/nsmc/'\n",
    "# data_path = work_dir + 'train.csv'\n",
    "contents, points = tool.loading_rdata(files, eng=True, num=True, punc=False)\n",
    "contents = tool.cut(contents,cut=2)\n",
    "\n",
    "# tranform document to vector\n",
    "max_document_length = 50\n",
    "x, vocabulary, vocab_size = tool.make_input(contents,max_document_length)\n",
    "print('사전단어수 : %s' % (vocab_size))\n",
    "y = tool.make_output(points,threshold=0.5)\n",
    "\n",
    "# divide dataset into train/test set\n",
    "x_train, x_test, y_train, y_test = tool.divide(x,y,train_prop=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149907"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         - sequence_length: 최대 문장 길이\n",
    "#         - num_classes: 클래스 개수\n",
    "#         - vocab_size: 등장 단어 수\n",
    "#         - embedding_size: 각 단어에 해당되는 임베디드 벡터의 차원\n",
    "#         - filter_sizes: convolutional filter들의 사이즈 (= 각 filter가 몇 개의 단어를 볼 것인가?) (예: \"3, 4, 5\")\n",
    "#         - num_filters: 각 filter size 별 filter 수\n",
    "#         - l2_reg_lambda: 각 weights, biases에 대한 l2 regularization 정도\n",
    "\n",
    "sequence_length = max_document_length\n",
    "num_classes = 2\n",
    "embedding_size = 64\n",
    "filter_sizes = 3\n",
    "num_filters = 128\n",
    "l2_reg_lambda = 0\n",
    "none = None\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "evaluate_every = 100\n",
    "dropout_keep_p = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for input, output and dropout\n",
    "tf.reset_default_graph()\n",
    "input_x = tf.placeholder(tf.int32, [none, sequence_length], name=\"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [none, num_classes], name=\"input_y\")\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "l2_loss = tf.constant(0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "WARNING:tensorflow:From <ipython-input-14-051bbb64940f>:43: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-14-051bbb64940f>:62: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#with tf.device('/gpu:0'), tf.name_scope(\"embedding\"):\n",
    "with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "    W = tf.Variable(\n",
    "        tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "        name=\"W\")\n",
    "    embedded_chars = tf.nn.embedding_lookup(W, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "                                             \n",
    "\n",
    "pooled_outputs = []\n",
    "filter_size = filter_sizes\n",
    "with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "    # Convolution Layer\n",
    "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "    conv = tf.nn.conv2d(\n",
    "        embedded_chars_expanded,\n",
    "        W,\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv\")\n",
    "    # Apply nonlinearity\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "    # Maxpooling over the outputs\n",
    "    pooled = tf.nn.max_pool(\n",
    "        h,\n",
    "        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool\")\n",
    "    pooled_outputs.append(pooled)\n",
    "                                             \n",
    "\n",
    "# Combine all the pooled features\n",
    "num_filters_total = num_filters * 1 #len(filter_sizes)\n",
    "print(num_filters_total)\n",
    "h_pool = tf.concat(pooled_outputs, axis=1)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "\n",
    "# Add dropout\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "\n",
    "\n",
    "# Final (unnormalized) scores and predictions\n",
    "with tf.name_scope(\"output\"):\n",
    "#     W = tf.get_variable(\n",
    "#         \"W\",\n",
    "#         shape=[num_filters_total, num_classes],\n",
    "#         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "    l2_loss += tf.nn.l2_loss(W)\n",
    "    l2_loss += tf.nn.l2_loss(b)\n",
    "    scores = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\n",
    "    predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "\n",
    "# Calculate Mean cross-entropy loss\n",
    "\n",
    "#with tf.name_scope(\"loss\"):\n",
    "losses = tf.nn.softmax_cross_entropy_with_logits(labels=input_y, logits=scores)\n",
    "loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "# Accuracy\n",
    "#with tf.name_scope(\"accuracy\"):\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training procedure\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(0.0002)\n",
    "train_op = optimizer.minimize(loss)\n",
    "#train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug  <generator object batch_iter at 0x7f4c0e208410>\n",
      "num_batches_per_epoch 9994\n",
      "************epoch number*************  0\n",
      "loss 0.6543869972229004 acc 0.6875\n",
      "#################################test loss 1.216841459274292 acc 0.4375\n",
      "loss 0.9035086342079672 acc 0.5081087824351297\n",
      "loss 0.8514387085066213 acc 0.520541958041958\n",
      "#################################test loss 0.8007152676582336 acc 0.51171875\n",
      "loss 0.8154151566937159 acc 0.5330196535642905\n",
      "loss 0.7871399691913915 acc 0.5446964017991005\n",
      "#################################test loss 0.6875177621841431 acc 0.56640625\n",
      "loss 0.7652619268025745 acc 0.5541783286685326\n",
      "loss 0.7479073910544769 acc 0.5632914028657114\n",
      "#################################test loss 0.6546580791473389 acc 0.6328125\n",
      "loss 0.7329985172331112 acc 0.5714974293059126\n",
      "loss 0.7195551649566532 acc 0.5805736065983504\n",
      "#################################test loss 0.6123121976852417 acc 0.67578125\n",
      "loss 0.7071109576454112 acc 0.5901049766718507\n",
      "loss 0.6964075358789745 acc 0.5980928814237153\n",
      "#################################test loss 0.6068693399429321 acc 0.6640625\n",
      "loss 0.6865494760886645 acc 0.6059125613524814\n",
      "loss 0.6772203615726461 acc 0.6128978503582736\n",
      "#################################test loss 0.5307741761207581 acc 0.75390625\n",
      "loss 0.6686341274822516 acc 0.6196546685125366\n",
      "loss 0.6597323693976779 acc 0.6264462219682903\n",
      "#################################test loss 0.5719665288925171 acc 0.6796875\n",
      "loss 0.6520485886890496 acc 0.6321657112385015\n",
      "loss 0.6444123562366243 acc 0.6383655168103987\n",
      "#################################test loss 0.5754916071891785 acc 0.69140625\n",
      "loss 0.6380185413754641 acc 0.6435860487001529\n",
      "loss 0.630769783565368 acc 0.6490112209754472\n",
      "#################################test loss 0.46906358003616333 acc 0.7890625\n",
      "loss 0.624956021033429 acc 0.6536877697084518\n",
      "************epoch number*************  1\n",
      "loss 0.5132027268409729 acc 0.8125\n",
      "#################################test loss 0.4501083493232727 acc 0.75390625\n",
      "loss 0.4844253210726374 acc 0.7603542914171657\n",
      "loss 0.4815033333344417 acc 0.7629245754245755\n",
      "#################################test loss 0.5117707252502441 acc 0.7734375\n",
      "loss 0.483660691305449 acc 0.7616172551632245\n",
      "loss 0.4830224435666631 acc 0.7639617691154422\n",
      "#################################test loss 0.47017353773117065 acc 0.78515625\n",
      "loss 0.4800032219651793 acc 0.7658186725309876\n",
      "loss 0.4782223213725946 acc 0.7674941686104632\n",
      "#################################test loss 0.4974537491798401 acc 0.73828125\n",
      "loss 0.4773358474609274 acc 0.7680127106540988\n",
      "loss 0.4732967574904484 acc 0.7709010247438141\n",
      "#################################test loss 0.5105646848678589 acc 0.7578125\n",
      "loss 0.4697575898560861 acc 0.7729393468118196\n",
      "loss 0.46751072786195685 acc 0.7742826434713057\n",
      "#################################test loss 0.5311281085014343 acc 0.75390625\n",
      "loss 0.46598221398206047 acc 0.7752454099254681\n",
      "loss 0.46498977105243267 acc 0.775964422596234\n",
      "#################################test loss 0.4674355685710907 acc 0.765625\n",
      "loss 0.4639370424658349 acc 0.7763709429318566\n",
      "loss 0.46343314418867987 acc 0.7766301242679617\n",
      "#################################test loss 0.3736635446548462 acc 0.8359375\n",
      "loss 0.46247234875369814 acc 0.7772380349286762\n",
      "loss 0.4611979774342017 acc 0.7780199350081239\n",
      "#################################test loss 0.44234463572502136 acc 0.796875\n",
      "loss 0.46023891690919966 acc 0.7788127867309729\n",
      "loss 0.4598918044143962 acc 0.7792259193422953\n",
      "#################################test loss 0.4538235068321228 acc 0.78515625\n",
      "loss 0.4586536661817731 acc 0.7798455425744658\n",
      "************epoch number*************  2\n",
      "loss 0.3362007737159729 acc 0.8125\n",
      "#################################test loss 0.49509474635124207 acc 0.7734375\n",
      "loss 0.4163027953453169 acc 0.8083832335329342\n",
      "loss 0.41609691888778716 acc 0.8096278721278721\n",
      "#################################test loss 0.4294136166572571 acc 0.7734375\n",
      "loss 0.4180441816733886 acc 0.8090023317788141\n",
      "loss 0.41593662920831026 acc 0.8085019990004998\n",
      "#################################test loss 0.43999746441841125 acc 0.7890625\n",
      "loss 0.4178100865079564 acc 0.8072021191523391\n",
      "loss 0.4172819505275905 acc 0.8080848050649784\n",
      "#################################test loss 0.4623444676399231 acc 0.79296875\n",
      "loss 0.41531435597950783 acc 0.8093937446443873\n",
      "loss 0.41350332950166213 acc 0.8106254686328418\n",
      "#################################test loss 0.43758106231689453 acc 0.828125\n",
      "loss 0.4124454167754061 acc 0.8109447900466563\n",
      "loss 0.41087174647553065 acc 0.8119376124775045\n",
      "#################################test loss 0.5227512121200562 acc 0.76171875\n",
      "loss 0.40954814452953586 acc 0.8124772768587529\n",
      "loss 0.4091779572668522 acc 0.8125416597233794\n",
      "#################################test loss 0.4411313533782959 acc 0.80859375\n",
      "loss 0.40856751843363187 acc 0.81240386094447\n",
      "loss 0.4071420472045572 acc 0.8133838023139551\n",
      "#################################test loss 0.45735806226730347 acc 0.80859375\n",
      "loss 0.4066284588655938 acc 0.8138748166911078\n",
      "loss 0.4065385209167172 acc 0.813921697287839\n",
      "#################################test loss 0.46452945470809937 acc 0.80859375\n",
      "loss 0.4067931834758625 acc 0.8137204446535702\n",
      "loss 0.40570661804286284 acc 0.8146803132985224\n",
      "#################################test loss 0.43060433864593506 acc 0.79296875\n",
      "loss 0.4060158594625647 acc 0.8143616461425113\n",
      "************epoch number*************  3\n",
      "loss 0.19068953394889832 acc 0.9375\n",
      "#################################test loss 0.43840426206588745 acc 0.80859375\n",
      "loss 0.38399404781069346 acc 0.8217315369261478\n",
      "loss 0.381566154231856 acc 0.8257992007992008\n",
      "#################################test loss 0.36106961965560913 acc 0.84375\n",
      "loss 0.3807732900635629 acc 0.8268654230512992\n",
      "loss 0.3808833922611839 acc 0.8269615192403799\n",
      "#################################test loss 0.45263609290122986 acc 0.78125\n",
      "loss 0.3797676644650329 acc 0.8291683326669332\n",
      "loss 0.37897862267073135 acc 0.8301816061312896\n",
      "#################################test loss 0.45328158140182495 acc 0.7890625\n",
      "loss 0.38035800674240167 acc 0.829620108540417\n",
      "loss 0.37952135419494004 acc 0.8298081729567608\n",
      "#################################test loss 0.38506871461868286 acc 0.82421875\n",
      "loss 0.37934316906782556 acc 0.8298155965341035\n",
      "loss 0.3790398618518126 acc 0.8303589282143571\n",
      "#################################test loss 0.4541686177253723 acc 0.7734375\n",
      "loss 0.3789661890256407 acc 0.8304399200145428\n",
      "loss 0.3790459820596879 acc 0.8302991168138644\n",
      "#################################test loss 0.38878265023231506 acc 0.81640625\n",
      "loss 0.37912175589832886 acc 0.8299107829564683\n",
      "loss 0.37944614190494924 acc 0.8295422082559635\n",
      "#################################test loss 0.49069473147392273 acc 0.76171875\n",
      "loss 0.38017856806055766 acc 0.8291644447407013\n",
      "loss 0.37918885525722473 acc 0.8297165979252593\n",
      "#################################test loss 0.513077437877655 acc 0.75390625\n",
      "loss 0.3791072009651258 acc 0.8299023644277144\n",
      "loss 0.37903450732147836 acc 0.8300119431174314\n",
      "#################################test loss 0.4011080265045166 acc 0.83203125\n",
      "loss 0.37917317215966095 acc 0.8302349752657615\n",
      "************epoch number*************  4\n",
      "loss 0.375891774892807 acc 0.75\n",
      "#################################test loss 0.4385761320590973 acc 0.80859375\n",
      "loss 0.35259810708716005 acc 0.844935129740519\n",
      "loss 0.3542042663553497 acc 0.8457167832167832\n",
      "#################################test loss 0.4750021994113922 acc 0.8046875\n",
      "loss 0.3584202721883741 acc 0.8433544303797469\n",
      "loss 0.359742545835052 acc 0.8418603198400799\n",
      "#################################test loss 0.4961409568786621 acc 0.78125\n",
      "loss 0.36095378727840927 acc 0.8413134746101559\n",
      "loss 0.36033288653603557 acc 0.8419901699433522\n",
      "#################################test loss 0.4388250410556793 acc 0.80859375\n",
      "loss 0.35838134198427474 acc 0.8432412167952014\n",
      "loss 0.35857824579726694 acc 0.8431485878530367\n",
      "#################################test loss 0.45281997323036194 acc 0.7734375\n",
      "loss 0.36000380714392294 acc 0.8421323039324594\n",
      "loss 0.35881381092298464 acc 0.8426814637072585\n",
      "#################################test loss 0.44685298204421997 acc 0.80859375\n",
      "loss 0.35843465789283024 acc 0.8432216869660062\n",
      "loss 0.3577602564322613 acc 0.8434010998166972\n",
      "#################################test loss 0.4246838390827179 acc 0.80859375\n",
      "loss 0.3584222493011609 acc 0.8430914474696201\n",
      "loss 0.3584049396885197 acc 0.8430581345522068\n",
      "#################################test loss 0.4426538348197937 acc 0.83203125\n",
      "loss 0.35851392563178675 acc 0.8429959338754832\n",
      "loss 0.35903852659295893 acc 0.8427540307461567\n",
      "#################################test loss 0.4807968735694885 acc 0.76953125\n",
      "loss 0.3598052268365126 acc 0.8423788377837902\n",
      "loss 0.359961771478558 acc 0.8424966670369959\n",
      "#################################test loss 0.3098357319831848 acc 0.859375\n",
      "loss 0.3600994673003844 acc 0.8422863382801811\n",
      "###############################################total test loss 0.024365307625573065 acc 0.82641389888494\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# 3. train the model and test\n",
    "#with tf.Graph().as_default():\n",
    "session_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config=session_config)\n",
    "#sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "        \"\"\"\n",
    "        Generates a batch iterator for a dataset.\n",
    "        \"\"\"\n",
    "        data = np.array(data)\n",
    "        data_size = len(data)\n",
    "        num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                yield shuffled_data[start_index:end_index]\n",
    "\n",
    "    total_num = len(list(zip(x_train, y_train)))\n",
    "    num_batches_per_epoch = int((total_num - 1)/batch_size) +1\n",
    "\n",
    "    batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "    \n",
    "    test_batches = batch_iter(\n",
    "            list(zip(x_test, y_test)), 256, num_epochs=1)    \n",
    "    \n",
    "    print(\"debug \",batches) # batches is generator\n",
    "\n",
    "    print(\"num_batches_per_epoch\", num_batches_per_epoch)\n",
    "    #sys.exit()\n",
    "    for epoch in range(num_epochs):\n",
    "        totloss = 0\n",
    "        totacc = 0\n",
    "        print(\"************epoch number************* \", epoch)\n",
    "        for iter in range(num_batches_per_epoch):\n",
    "            denom  = iter+1\n",
    "            x_batch, y_batch = zip(*next(batches)) \n",
    "            #print(\"debug: \", y_batch)\n",
    "            x_batch = np.array(x_batch)\n",
    "            y_batch = np.array(y_batch)\n",
    "            #print(\"debug: \", type(y_batch))\n",
    "\n",
    "            feed_dict = {\n",
    "                input_x: x_batch,\n",
    "                input_y: y_batch,\n",
    "                dropout_keep_prob: dropout_keep_p\n",
    "            }   \n",
    "            _, step, loss1, accuracy1 = sess.run(\n",
    "                [train_op, global_step,  loss, accuracy], feed_dict) \n",
    "            totloss += loss1\n",
    "            totacc += accuracy1\n",
    "            if iter % 500 == 0:\n",
    "                print(\"loss {} acc {}\".format(totloss/denom, totacc/denom))\n",
    "            if iter % 1000 == 0:\n",
    "                x_batch, y_batch = zip(*next(test_batches)) \n",
    "                x_batch = np.array(x_batch)\n",
    "                y_batch = np.array(y_batch)    \n",
    "                feed_dict = {\n",
    "                    input_x: x_batch,\n",
    "                    input_y: y_batch,\n",
    "                    dropout_keep_prob: dropout_keep_p\n",
    "                }  \n",
    "                _, step, loss1, accuracy1 = sess.run(\n",
    "                    [train_op, global_step,  loss, accuracy], feed_dict)  \n",
    "                print(\"#################################test loss {} acc {}\".format(loss1, accuracy1))\n",
    "                \n",
    "    \n",
    "    test_batches = batch_iter(\n",
    "            list(zip(x_test, y_test)), batch_size, num_epochs=1)   \n",
    "    total_num_batch = len(list(zip(x_test, y_test)))\n",
    "    num_batches_per_epoch = int((total_num_batch - 1)/batch_size) +1\n",
    "    totacc_test = 0\n",
    "    totloss_test = 0\n",
    "    for iter in range(num_batches_per_epoch):\n",
    "        x_batch, y_batch = zip(*next(test_batches)) \n",
    "        x_batch = np.array(x_batch)\n",
    "        y_batch = np.array(y_batch)    \n",
    "        feed_dict = {\n",
    "            input_x: x_batch,\n",
    "            input_y: y_batch,\n",
    "            dropout_keep_prob: 1\n",
    "        }     \n",
    "        _, step, loss1, accuracy1 = sess.run(\n",
    "            [train_op, global_step,  loss, accuracy], feed_dict)  \n",
    "        totacc_test += accuracy1\n",
    "        totloss_test += loss1\n",
    "        \n",
    "    print(\"###############################################total test loss {} acc {}\".format(\n",
    "        totloss_test/total_num_batch, totacc_test/num_batches_per_epoch))            \n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(*batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
